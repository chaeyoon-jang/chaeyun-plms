{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/chaeyun-jang/.conda/envs/jax-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_text_field_map = {\n",
    "     'cola': ['sentence'],\n",
    "     'sst2': ['sentence'],\n",
    "     'mrpc': ['sentence1', 'sentence2'],\n",
    "     'qqp': ['question1', 'question2'],\n",
    "     'stsb': ['sentence1', 'sentence2'],\n",
    "     'mnli': ['premise', 'hypothesis'],\n",
    "     'qnli': ['question', 'sentence'],\n",
    "     'rte': ['sentence1', 'sentence2'],\n",
    "     'wnli': ['sentence1', 'sentence2']}\n",
    "\n",
    "glue_task_num_labels = {\n",
    "     'cola': 2, 'sst2': 2,\n",
    "     'mrpc': 2, 'qqp': 2,\n",
    "     'stsb': 1, 'mnli': 3,\n",
    "     'qnli': 2, 'rte': 2,\n",
    "     'wnli': 2}\n",
    "\n",
    "loader_columns = [\n",
    "     'input_ids',\n",
    "     'token_type_ids',\n",
    "     'attention_mask',\n",
    "     'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = dict()\n",
    "for name in list(task_text_field_map.keys()):\n",
    "    dataframes[name] = datasets.load_dataset('glue', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cola': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence', 'label', 'idx'],\n",
       "         num_rows: 8551\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['sentence', 'label', 'idx'],\n",
       "         num_rows: 1043\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['sentence', 'label', 'idx'],\n",
       "         num_rows: 1063\n",
       "     })\n",
       " }),\n",
       " 'sst2': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence', 'label', 'idx'],\n",
       "         num_rows: 67349\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['sentence', 'label', 'idx'],\n",
       "         num_rows: 872\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['sentence', 'label', 'idx'],\n",
       "         num_rows: 1821\n",
       "     })\n",
       " }),\n",
       " 'mrpc': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 3668\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 408\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 1725\n",
       "     })\n",
       " }),\n",
       " 'qqp': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['question1', 'question2', 'label', 'idx'],\n",
       "         num_rows: 363846\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['question1', 'question2', 'label', 'idx'],\n",
       "         num_rows: 40430\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['question1', 'question2', 'label', 'idx'],\n",
       "         num_rows: 390965\n",
       "     })\n",
       " }),\n",
       " 'stsb': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 5749\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 1500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 1379\n",
       "     })\n",
       " }),\n",
       " 'mnli': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 392702\n",
       "     })\n",
       "     validation_matched: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 9815\n",
       "     })\n",
       "     validation_mismatched: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 9832\n",
       "     })\n",
       "     test_matched: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 9796\n",
       "     })\n",
       "     test_mismatched: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 9847\n",
       "     })\n",
       " }),\n",
       " 'qnli': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['question', 'sentence', 'label', 'idx'],\n",
       "         num_rows: 104743\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['question', 'sentence', 'label', 'idx'],\n",
       "         num_rows: 5463\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['question', 'sentence', 'label', 'idx'],\n",
       "         num_rows: 5463\n",
       "     })\n",
       " }),\n",
       " 'rte': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 2490\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 277\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 3000\n",
       "     })\n",
       " }),\n",
       " 'wnli': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 635\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 71\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 146\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.39MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(example_batch):\n",
    "    if len(text_fields) > 1:\n",
    "        texts_or_text_pairs = list(zip(example_batch[text_fields[0]], \n",
    "                                               example_batch[text_fields[1]]))\n",
    "    else:\n",
    "        texts_or_text_pairs = example_batch[text_fields[0]]\n",
    "                \n",
    "    features = tokenizer.batch_encode_plus(texts_or_text_pairs, add_special_tokens=True) \n",
    "            \n",
    "    features['label'] = example_batch['label']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   2%|▏         | 2000/104743 [00:00<00:11, 8894.38 examples/s]   Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "for name in dataframes.keys():\n",
    "    text_fields = task_text_field_map[name]\n",
    "    #num_labels = glue_task_num_labels[name]\n",
    "    dataframe = dataframes[name]\n",
    "    for split in dataframe.keys():\n",
    "        dataframe[split] = dataframe[split].map(convert_to_features, batched=True)\n",
    "        columns = [c for c in dataframe[split].column_names if c in loader_columns]\n",
    "        dataframe[split].set_format(type=\"torch\", columns=columns)\n",
    "    dataframes[name] = dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already done.\n",
      "already done.\n",
      "already done.\n",
      "already done.\n",
      "already done.\n",
      "already done.\n",
      "already done.\n",
      "already done.\n",
      "already done.\n"
     ]
    }
   ],
   "source": [
    "for name in dataframes.keys():\n",
    "    try:\n",
    "        text_fields = task_text_field_map[name] + ['idx']\n",
    "        dataframes[name] = dataframes[name].remove_columns(text_fields)\n",
    "    except:\n",
    "        print('already done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cola': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 8551\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['label', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 1043\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['label', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 1063\n",
       "     })\n",
       " }),\n",
       " 'sst2': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 67349\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 872\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 1821\n",
       "     })\n",
       " }),\n",
       " 'mrpc': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 3668\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 408\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 1725\n",
       "     })\n",
       " }),\n",
       " 'qqp': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 363846\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 40430\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 390965\n",
       "     })\n",
       " }),\n",
       " 'stsb': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 5749\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 1500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 1379\n",
       "     })\n",
       " }),\n",
       " 'mnli': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 392702\n",
       "     })\n",
       "     validation_matched: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 9815\n",
       "     })\n",
       "     validation_mismatched: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 9832\n",
       "     })\n",
       "     test_matched: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 9796\n",
       "     })\n",
       "     test_mismatched: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 9847\n",
       "     })\n",
       " }),\n",
       " 'qnli': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 104743\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 5463\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 5463\n",
       "     })\n",
       " }),\n",
       " 'rte': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 2490\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 277\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 3000\n",
       "     })\n",
       " }),\n",
       " 'wnli': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 635\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 71\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['label', 'idx', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 146\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def summary_data(dataframe):\n",
    "    def process(sub_df):\n",
    "        temp_dict = dict()\n",
    "        temp_dict['label'] = list(sub_df['label'])\n",
    "        temp_dict['sent_length'] = [len(x) for x in sub_df['input_ids']]\n",
    "        return pandas.DataFrame(temp_dict)\n",
    "    temp_dict = [process(dataframe[name]) for name in dataframe.keys()]\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [summary_data(dataframes[name]) for name in dataframes.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(df):\n",
    "    df['label'] = [int(x) for x in list(df['label'])] \n",
    "    print(int(df.describe()['sent_length']['max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cola data summary\n",
      "train set\n",
      "47\n",
      "validation set\n",
      "36\n",
      "test set\n",
      "34\n",
      "================\n",
      "sst2 data summary\n",
      "train set\n",
      "67\n",
      "validation set\n",
      "63\n",
      "test set\n",
      "63\n",
      "================\n",
      "mrpc data summary\n",
      "train set\n",
      "104\n",
      "validation set\n",
      "84\n",
      "test set\n",
      "99\n",
      "================\n",
      "qqp data summary\n",
      "train set\n",
      "317\n",
      "validation set\n",
      "191\n",
      "test set\n",
      "471\n",
      "================\n",
      "stsb data summary\n",
      "train set\n",
      "125\n",
      "validation set\n",
      "91\n",
      "test set\n",
      "82\n",
      "================\n",
      "mnli data summary\n",
      "train set\n",
      "425\n",
      "validation set\n",
      "229\n",
      "test set\n",
      "220\n",
      "================\n",
      "qnli data summary\n",
      "train set\n",
      "566\n",
      "validation set\n",
      "259\n",
      "test set\n",
      "309\n",
      "================\n",
      "rte data summary\n",
      "train set\n",
      "292\n",
      "validation set\n",
      "248\n",
      "test set\n",
      "248\n",
      "================\n",
      "wnli data summary\n",
      "train set\n",
      "109\n",
      "validation set\n",
      "107\n",
      "test set\n",
      "99\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "for tdx, name in enumerate(dataframes.keys()):\n",
    "    print(f'{name} data summary')\n",
    "    for idx, type in enumerate(['train','validation','test']):\n",
    "        print(f'{type} set')\n",
    "        analyze_data(df_list[tdx][idx])\n",
    "    print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label  sent_length\n",
      "2711       0          565\n",
      "82398      0          566\n",
      "82549      0          565\n",
      "84277      0          564\n",
      "88427      0          564\n",
      "total df length : 104743\n"
     ]
    }
   ],
   "source": [
    "for sub_df in df_list[6]:\n",
    "    #print(sub_df.describe())\n",
    "    print(sub_df[sub_df['sent_length'] > 512])\n",
    "    print(f'total df length : {len(sub_df)}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
